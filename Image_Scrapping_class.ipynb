{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A scrapper to extract images from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv \n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleScrapper:\n",
    "    base_url = 'https://www.google.com/search?'\n",
    "    base_url2 = 'https://www.google.com'\n",
    "\n",
    "    \n",
    "    params_1 = {\n",
    "        'q':'',\n",
    "        'tbm':'isch',\n",
    "        \"ijn\": \"1\",\n",
    "    }\n",
    "\n",
    "    params_2 = {\n",
    "        'q':'',\n",
    "        'tbm':'isch',\n",
    "        'ved':'2ahUKEwi0sJ-Ji-b0AhWt34UKHQkOC_8Q2-cCegQIABAA',\n",
    "        'oq':'',\n",
    "        'gs_lcp':'CgNpbWcQDDIECAAQQzIGCAAQBxAeMgYIABAHEB4yBggAEAcQHjIGCAAQBxAeMgYIABAHEB4yBAgAEB4yBAgAEB4yBAgAEB4yBAgAEB5QAFgAYOoHaABwAHgAgAE-iAE-kgEBMZgBAKoBC2d3cy13aXotaW1nwAEB',\n",
    "        'sclient':'img',\n",
    "        'ei':'JwW6YfSAJq2_lwSJnKz4Dw',\n",
    "        'bih':'1000',\n",
    "        'biw':'1000',\n",
    "        'client':'avast-a-1'\n",
    "    }\n",
    "\n",
    "    params=params_2\n",
    "\n",
    "    headers = {\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'cookie':'CONSENT=YES+srp.gws-20210913-0-RC2.fr+FX+840; SEARCH_SAMESITE=CgQI3ZMB; HSID=Aq7Ex0RHNR5IAXW8o; SSID=A52P5roTjlYkSZbDG; APISID=Ulcpkn_C81XbYb-Z/AuVDgAuUHdw6pfs6L; SAPISID=WMB3xWUxTleu0jD7/AhyEqgniw-Pm17oZ5; __Secure-1PAPISID=WMB3xWUxTleu0jD7/AhyEqgniw-Pm17oZ5; __Secure-3PAPISID=WMB3xWUxTleu0jD7/AhyEqgniw-Pm17oZ5; SID=Ewjlvqv04tWhaCdcn9KDaBNnZprrMYZ5ugMyMEahMK7cPCW5PftVaOCeCk0qVp99ph88iw.; __Secure-1PSID=Ewjlvqv04tWhaCdcn9KDaBNnZprrMYZ5ugMyMEahMK7cPCW5T3gKD42x6rX5XjaBWS0p2A.; __Secure-3PSID=Ewjlvqv04tWhaCdcn9KDaBNnZprrMYZ5ugMyMEahMK7cPCW5rjCxHmKnl2bAB5B3Qsx6vA.; OGPC=19025836-2:19022552-1:; NID=511=Y76HZ-UEU2AbiFK3nXaKxXD1OYMOezcBQNBzAJ214rSVrWJj396uM_l5ThV33CPNm1WD23tl-YWvIGEEncm8Tp2eCGyG2nyor1hFVuqf9BZ8i8wva8TTsrDfheF8hrPIpFo_-Xram8Dg-wyDCdvvDndAB93rEr4_lgDBz9Ulr_FGu1m6AWhOOGg3Xgf7UVVlwrlAzUDSF3k8HuDGPzaUxtd96IPMMTDFB1EaZ0bALxF73bRZPHWbhEyaLszjV72AIZVmtVXL1dYJ2K2ad9cfZ0nw62U_fyTUT8eS8kiTevssPOZKDWTd7RXi3lAPsZnOb2L8BLhsK4oGF4juKI1jRB5bKExy94Pidd2ETFcMJTE; DV=g5o3SPEiFA1NMO5HzDUyvBPl-pcD4pdw4P5MyRCtAwEAAPAOYbpZMjuKlgAAAGDtw7_cZzBvJwAAAA; 1P_JAR=2022-1-3-13; OTZ=6315236_52_52_123900_48_436380; SIDCC=AJi4QfFOI1mHQKuqF3cTJuJCa0UP03Urg7O6yTrt3EhXdB6hgBFzae2y00YqJJy0rxgMlaB31g; __Secure-3PSIDCC=AJi4QfFXmV6CGPzb9YnvDQhLS_bsf9F4yX_CnEEzZb1rH8fhRuRvF_Ryqpw-C1KzkQeZrlt8EA',\n",
    "        'dnt': '1',\n",
    "        'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"96\", \"Avast Secure Browser\";v=\"96\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'none',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Avast/96.1.13589.111'\n",
    "    }\n",
    "\n",
    "    \n",
    "    def fetch_page(self,query='',next=False,img_urls=False,url=''):\n",
    "        if url=='':\n",
    "            self.params['q']=query\n",
    "            response = requests.get(self.base_url,params=self.params,headers= self.headers)\n",
    "\n",
    "        else:\n",
    "            response = requests.get(url)\n",
    "\n",
    "        nextpage = 0\n",
    "        urls=[]\n",
    "        if next or img_urls :\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            if next :\n",
    "                for i in soup.find_all('a', href = True,class_= \"frGj1b\"):\n",
    "                    nextpage = self.base_url2+i['href']\n",
    "            if img_urls:\n",
    "                for i in soup.find_all('img',class_= \"yWs4tf\"):\n",
    "                    urls.append(i['src'])\n",
    "        return (response,nextpage,urls)\n",
    "\n",
    "\n",
    "\n",
    "    def store_response(self, response, filename):\n",
    "        if response.status_code == 200:\n",
    "            print('Saving response to \"res.html\"')\n",
    "            \n",
    "            with open(filename+'.html','w') as html_file:\n",
    "                html_file.write(response.text)\n",
    "\n",
    "            print('Done')\n",
    "        else :\n",
    "            print('Bad response!')\n",
    "\n",
    "\n",
    "\n",
    "    def fetch_image_urls_from_files(self,dirname,nb_images_min):\n",
    "        urls=[]\n",
    "        if len(dirname)==0:\n",
    "            files = [f for f in os.listdir() if os.path.isfile(f) and f[-5:]=='.html']\n",
    "            racine=True\n",
    "        else :\n",
    "            files = [f for f in os.listdir(dirname) if os.path.isfile(os.path.join(dirname, f)) and f[-5:]=='.html']\n",
    "            racine=False\n",
    "\n",
    "        file_max=len(files)\n",
    "        file_num=0\n",
    "        nb_urls=0\n",
    "\n",
    "        while file_num < file_max and nb_urls < nb_images_min :\n",
    "            if racine :\n",
    "                path = files[file_num]\n",
    "            else :\n",
    "                path=dirname+'/'+ files[file_num]\n",
    "            with open(path) as fp:\n",
    "                soup = BeautifulSoup(fp, 'html.parser')\n",
    "                for i in soup.find_all('img',class_= \"yWs4tf\"):\n",
    "                    urls.append(i['src'])\n",
    "            file_num+=1\n",
    "            nb_urls=len(urls)\n",
    "        \n",
    "        return urls,nb_urls\n",
    "\n",
    "\n",
    "\n",
    "    def fetch_image_urls_from_web(self,query,nb_images_min):\n",
    "        \n",
    "        #Initialisations :\n",
    "        dirname0=query.replace(' ','_').lower()\n",
    "        dirname1=dirname0+'/htmls'\n",
    "        urls=[] #Initialisation de la liste des urls\n",
    "        p=1 #Initialisation du compteurs de pages  télécharger\n",
    "        nb_urls=0\n",
    "        next_page_url=''\n",
    "\n",
    "        #Creation of directories\n",
    "        if not os.path.isdir(dirname0):\n",
    "            os.mkdir(dirname0)\n",
    "        if not os.path.isdir(dirname1):\n",
    "            os.mkdir(dirname1)\n",
    "\n",
    "        #fetching urls from each page and storing page\n",
    "        while nb_urls < nb_images_min :\n",
    "            filename=dirname1+'/'+dirname0+'_'+str(p)+'.html'\n",
    "            response,next_page_url,urls_temp=self.fetch_page(query,next=True,img_urls=True,url=next_page_url)\n",
    "            self.store_response(response,filename)\n",
    "            urls+=urls_temp\n",
    "            nb_urls=len(urls)\n",
    "            p+=1\n",
    "        return urls,nb_urls\n",
    "\n",
    "\n",
    "    def load_img_urls(self,query,nb_images_min):\n",
    "\n",
    "        # load urls from directory first then fetch from web\n",
    "        dirname0=query.replace(' ','_').lower()\n",
    "        dirname1=dirname0+'/htmls'\n",
    "        urls=[]\n",
    "        nb_urls=0\n",
    "\n",
    "        if os.path.isdir(dirname1):\n",
    "            urls,nb_urls=self.fetch_image_urls_from_files(dirname1,nb_images_min)\n",
    "        \n",
    "        if nb_urls < nb_images_min:\n",
    "            nb_images_min=nb_images_min-nb_urls\n",
    "            urls_temp,nb_urls_temp=self.fetch_image_urls_from_web(query,nb_images_min)\n",
    "            urls+=urls_temp\n",
    "            nb_urls+=nb_urls_temp\n",
    "        \n",
    "        return urls,nb_urls,dirname0\n",
    "\n",
    "\n",
    "    def download_imgs_from_urls(self,urls,dirname):\n",
    "        from tqdm import tqdm\n",
    "        #initialisation\n",
    "        p=1\n",
    "\n",
    "        print('start downloading')\n",
    "        for url in tqdm(urls):\n",
    "            filename=dirname+'/'+dirname+str(p)+'.jpeg'\n",
    "            urllib.request.urlretrieve(url,filename)\n",
    "            p+=1\n",
    "        \n",
    "        return print('{} images downloaded'.format(p-1))\n",
    "\n",
    "\n",
    "    def dowload_imgs_from_query(self,query,nb_images_min):\n",
    "        urls,_,dirname=self.load_img_urls(query,nb_images_min)\n",
    "        self.download_imgs_from_urls(urls,dirname)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def run(self,query,filename):\n",
    "        response,_,_ = self.fetch_page(query)\n",
    "        self.store_response(response,filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving response to \"res.html\"\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "scrapper = GoogleScrapper()\n",
    "scrapper.run('baba','Projet')\n",
    "scrapper.dowload_imgs_from_query('hoba hoba spirit',65)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c5c631493a37e7aa0ae8c444ee5537c3d5dfd0b691afcf1bd3205782a6b2fa5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('mlflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
